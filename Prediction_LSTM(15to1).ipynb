{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把所有年份的資料merge在一起,並存在data.csv裡\n",
    "def mergeData():\n",
    "    SaveFile_Name = 'data.csv'\n",
    "    file_list = os.listdir('data')\n",
    "    df = pd.read_csv('data'+'\\\\'+file_list[0])\n",
    "    df.to_csv(SaveFile_Name,encoding=\"utf_8_sig\",index=False)\n",
    "    for i in range(1,len(file_list)):\n",
    "        df = pd.read_csv('data'+'\\\\'+file_list[i])\n",
    "        df.to_csv(SaveFile_Name,encoding=\"utf_8_sig\",index=False, header=False, mode='a+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取data.csv\n",
    "def readData():\n",
    "    train = pd.read_csv(\"data.csv\")\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把年份換成西元年\n",
    "def changeYear(data):\n",
    "    for i in range(0,data.shape[0]):\n",
    "        Date=data[\"日期\"][i].split('/')\n",
    "        year,month,date=Date[0],Date[1],Date[2]\n",
    "        year=str(int(year)+1911)\n",
    "        data.loc[i,\"日期\"]=year+'/'+month+'/'+date\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加features(\"年\",\"月\",\"日\",\"第幾日\")\n",
    "def augFeatures(data):\n",
    "  data[\"日期\"] = pd.to_datetime(data[\"日期\"])\n",
    "  data[\"年\"] = data[\"日期\"].dt.year\n",
    "  data[\"月\"] = data[\"日期\"].dt.month\n",
    "  data[\"日\"] = data[\"日期\"].dt.day\n",
    "  data[\"第幾日\"] = data[\"日期\"].dt.dayofweek\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把非數字的資料換成正確資料,並減少features(\"日期\",\"成交股數\",\"成交金額\",等等...)\n",
    "def manage(data):\n",
    "    for i in range(0,data.shape[0]):\n",
    "        if data[\"漲跌價差\"][i]=='X0.00':\n",
    "            data.loc[i,\"漲跌價差\"]=str(int(data[\"收盤價\"][i])-int(data[\"收盤價\"][i-1]))\n",
    "    data=data.drop([\"日期\"], axis=1)\n",
    "    data=data.drop([\"成交股數\"], axis=1)\n",
    "    data=data.drop([\"成交金額\"], axis=1)\n",
    "    data=data.drop([\"漲跌價差\"], axis=1)\n",
    "    data=data.drop([\"成交筆數\"], axis=1)\n",
    "    data=data.convert_objects(convert_numeric=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把資料normalize\n",
    "def normalize(train):\n",
    "    train = train.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創造出train的資料,train_x為輸入資料(所有features),train_y為輸出資料(開盤價的成長率,分為9個區段)\n",
    "def buildTrain(train, pastDay=5, futureDay=1):\n",
    "    X_train, Y_train, Z_train= [], [], []\n",
    "    X,Y,Z=[],[],[]\n",
    "    for i in range(train.shape[0]-futureDay-pastDay):\n",
    "        X_train.append(np.array(train.iloc[i:i+pastDay]))\n",
    "        Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][\"開盤價\"]))\n",
    "        Z_train.append(np.array(train.iloc[i+pastDay-1:i+pastDay][\"開盤價\"]))\n",
    "    X=np.array(X_train)\n",
    "    Y=np.array(Y_train)\n",
    "    Z=np.array(Z_train)\n",
    "    Y=100*((Y-Z)/Z)\n",
    "    Y_train=[]\n",
    "    \n",
    "    for i in range(len(Y)):\n",
    "        if Y[i]<-3.5:\n",
    "            Y_train.append(np.array([0]))\n",
    "        elif -3.5<=Y[i]<-2.5:\n",
    "            Y_train.append(np.array([1]))\n",
    "        elif -2.5<=Y[i]<-1.5:\n",
    "            Y_train.append(np.array([2]))\n",
    "        elif -1.5<=Y[i]<-0.5:\n",
    "            Y_train.append(np.array([3]))\n",
    "        elif -0.5<=Y[i]<0.5:\n",
    "            Y_train.append(np.array([4]))\n",
    "        elif 0.5<=Y[i]<1.5:\n",
    "            Y_train.append(np.array([5]))\n",
    "        elif 1.5<=Y[i]<2.5:\n",
    "            Y_train.append(np.array([6]))\n",
    "        elif 2.5<=Y[i]<3.5:\n",
    "            Y_train.append(np.array([7]))\n",
    "        elif 3.5<=Y[i]:\n",
    "            Y_train.append(np.array([8]))\n",
    "    Y=np.array(Y_train)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把資料打亂\n",
    "def shuffle(X,Y):\n",
    "    np.random.seed()\n",
    "    randomList = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randomList)\n",
    "    return X[randomList], Y[randomList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將資料分成訓練資料和測試資料\n",
    "def splitData(X,Y,rate):\n",
    "    X_train = X[:-int(X.shape[0]*rate)]\n",
    "\n",
    "    Y_train = Y[:-int(Y.shape[0]*rate)]\n",
    "    \n",
    "    X_val = X[-int(X.shape[0]*rate):]\n",
    "\n",
    "    Y_val = Y[-int(Y.shape[0]*rate):]\n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立模型\n",
    "def buildModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_length=shape[1], input_dim=shape[2],return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(9)) \n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='adam',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    " \n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\admin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\admin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, input_shape=(5, 1), return_sequences=True)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 128)            66560     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9)                 1161      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 199,305\n",
      "Trainable params: 199,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5010 samples, validate on 557 samples\n",
      "Epoch 1/300\n",
      " - 3s - loss: 2.0869 - acc: 0.2293 - val_loss: 2.0261 - val_acc: 0.2478\n",
      "Epoch 2/300\n",
      " - 1s - loss: 2.0177 - acc: 0.2365 - val_loss: 2.0173 - val_acc: 0.2478\n",
      "Epoch 3/300\n",
      " - 1s - loss: 2.0180 - acc: 0.2401 - val_loss: 2.0171 - val_acc: 0.2478\n",
      "Epoch 4/300\n",
      " - 1s - loss: 2.0170 - acc: 0.2407 - val_loss: 2.0147 - val_acc: 0.2478\n",
      "Epoch 5/300\n",
      " - 1s - loss: 2.0166 - acc: 0.2399 - val_loss: 2.0309 - val_acc: 0.2478\n",
      "Epoch 6/300\n",
      " - 1s - loss: 2.0187 - acc: 0.2417 - val_loss: 2.0149 - val_acc: 0.2478\n",
      "Epoch 7/300\n",
      " - 1s - loss: 2.0154 - acc: 0.2419 - val_loss: 2.0148 - val_acc: 0.2478\n",
      "Epoch 8/300\n",
      " - 1s - loss: 2.0153 - acc: 0.2389 - val_loss: 2.0167 - val_acc: 0.2478\n",
      "Epoch 9/300\n",
      " - 1s - loss: 2.0151 - acc: 0.2413 - val_loss: 2.0163 - val_acc: 0.2478\n",
      "Epoch 10/300\n",
      " - 1s - loss: 2.0143 - acc: 0.2409 - val_loss: 2.0155 - val_acc: 0.2478\n",
      "Epoch 11/300\n",
      " - 1s - loss: 2.0164 - acc: 0.2395 - val_loss: 2.0162 - val_acc: 0.2478\n",
      "Epoch 12/300\n",
      " - 1s - loss: 2.0157 - acc: 0.2409 - val_loss: 2.0148 - val_acc: 0.2478\n",
      "Epoch 13/300\n",
      " - 1s - loss: 2.0147 - acc: 0.2423 - val_loss: 2.0158 - val_acc: 0.2478\n",
      "Epoch 14/300\n",
      " - 1s - loss: 2.0152 - acc: 0.2415 - val_loss: 2.0164 - val_acc: 0.2478\n",
      "Epoch 15/300\n",
      " - 1s - loss: 2.0149 - acc: 0.2405 - val_loss: 2.0150 - val_acc: 0.2478\n",
      "Epoch 16/300\n",
      " - 1s - loss: 2.0169 - acc: 0.2415 - val_loss: 2.0193 - val_acc: 0.2478\n",
      "Epoch 17/300\n",
      " - 1s - loss: 2.0156 - acc: 0.2403 - val_loss: 2.0138 - val_acc: 0.2478\n",
      "Epoch 18/300\n",
      " - 1s - loss: 2.0137 - acc: 0.2413 - val_loss: 2.0154 - val_acc: 0.2478\n",
      "Epoch 19/300\n",
      " - 1s - loss: 2.0150 - acc: 0.2411 - val_loss: 2.0160 - val_acc: 0.2478\n",
      "Epoch 20/300\n",
      " - 1s - loss: 2.0144 - acc: 0.2411 - val_loss: 2.0149 - val_acc: 0.2478\n",
      "Epoch 21/300\n",
      " - 1s - loss: 2.0142 - acc: 0.2413 - val_loss: 2.0137 - val_acc: 0.2478\n",
      "Epoch 22/300\n",
      " - 1s - loss: 2.0149 - acc: 0.2409 - val_loss: 2.0145 - val_acc: 0.2478\n",
      "Epoch 23/300\n",
      " - 1s - loss: 2.0142 - acc: 0.2419 - val_loss: 2.0134 - val_acc: 0.2478\n",
      "Epoch 24/300\n",
      " - 1s - loss: 2.0138 - acc: 0.2413 - val_loss: 2.0180 - val_acc: 0.2478\n",
      "Epoch 25/300\n",
      " - 1s - loss: 2.0149 - acc: 0.2395 - val_loss: 2.0136 - val_acc: 0.2478\n",
      "Epoch 26/300\n",
      " - 1s - loss: 2.0141 - acc: 0.2415 - val_loss: 2.0130 - val_acc: 0.2478\n",
      "Epoch 27/300\n",
      " - 1s - loss: 2.0133 - acc: 0.2415 - val_loss: 2.0156 - val_acc: 0.2478\n",
      "Epoch 28/300\n",
      " - 1s - loss: 2.0144 - acc: 0.2417 - val_loss: 2.0147 - val_acc: 0.2478\n",
      "Epoch 29/300\n",
      " - 1s - loss: 2.0145 - acc: 0.2405 - val_loss: 2.0156 - val_acc: 0.2478\n",
      "Epoch 30/300\n",
      " - 1s - loss: 2.0135 - acc: 0.2415 - val_loss: 2.0147 - val_acc: 0.2478\n",
      "Epoch 31/300\n",
      " - 1s - loss: 2.0126 - acc: 0.2415 - val_loss: 2.0131 - val_acc: 0.2478\n",
      "Epoch 32/300\n",
      " - 1s - loss: 2.0139 - acc: 0.2415 - val_loss: 2.0146 - val_acc: 0.2478\n",
      "Epoch 33/300\n",
      " - 1s - loss: 2.0139 - acc: 0.2415 - val_loss: 2.0145 - val_acc: 0.2478\n",
      "Epoch 34/300\n",
      " - 1s - loss: 2.0130 - acc: 0.2415 - val_loss: 2.0154 - val_acc: 0.2478\n",
      "Epoch 35/300\n",
      " - 1s - loss: 2.0126 - acc: 0.2415 - val_loss: 2.0144 - val_acc: 0.2478\n",
      "Epoch 36/300\n",
      " - 1s - loss: 2.0133 - acc: 0.2415 - val_loss: 2.0154 - val_acc: 0.2478\n",
      "Epoch 37/300\n",
      " - 1s - loss: 2.0132 - acc: 0.2415 - val_loss: 2.0140 - val_acc: 0.2478\n",
      "Epoch 38/300\n",
      " - 1s - loss: 2.0131 - acc: 0.2415 - val_loss: 2.0173 - val_acc: 0.2478\n",
      "Epoch 39/300\n",
      " - 1s - loss: 2.0134 - acc: 0.2417 - val_loss: 2.0157 - val_acc: 0.2478\n",
      "Epoch 40/300\n",
      " - 1s - loss: 2.0130 - acc: 0.2415 - val_loss: 2.0146 - val_acc: 0.2478\n",
      "Epoch 41/300\n",
      " - 1s - loss: 2.0125 - acc: 0.2415 - val_loss: 2.0144 - val_acc: 0.2478\n",
      "Epoch 42/300\n",
      " - 1s - loss: 2.0127 - acc: 0.2415 - val_loss: 2.0142 - val_acc: 0.2478\n",
      "Epoch 43/300\n",
      " - 1s - loss: 2.0140 - acc: 0.2413 - val_loss: 2.0142 - val_acc: 0.2478\n",
      "Epoch 44/300\n",
      " - 1s - loss: 2.0123 - acc: 0.2415 - val_loss: 2.0145 - val_acc: 0.2478\n",
      "Epoch 45/300\n",
      " - 1s - loss: 2.0115 - acc: 0.2415 - val_loss: 2.0152 - val_acc: 0.2478\n",
      "Epoch 46/300\n",
      " - 1s - loss: 2.0124 - acc: 0.2415 - val_loss: 2.0150 - val_acc: 0.2478\n",
      "Epoch 47/300\n",
      " - 1s - loss: 2.0110 - acc: 0.2415 - val_loss: 2.0183 - val_acc: 0.2478\n",
      "Epoch 48/300\n",
      " - 1s - loss: 2.0133 - acc: 0.2413 - val_loss: 2.0147 - val_acc: 0.2478\n",
      "Epoch 49/300\n",
      " - 1s - loss: 2.0123 - acc: 0.2415 - val_loss: 2.0149 - val_acc: 0.2478\n",
      "Epoch 50/300\n",
      " - 1s - loss: 2.0123 - acc: 0.2415 - val_loss: 2.0147 - val_acc: 0.2478\n",
      "Epoch 51/300\n",
      " - 1s - loss: 2.0129 - acc: 0.2415 - val_loss: 2.0140 - val_acc: 0.2478\n",
      "Epoch 52/300\n",
      " - 1s - loss: 2.0104 - acc: 0.2415 - val_loss: 2.0180 - val_acc: 0.2478\n",
      "Epoch 53/300\n",
      " - 1s - loss: 2.0127 - acc: 0.2415 - val_loss: 2.0160 - val_acc: 0.2478\n",
      "Epoch 54/300\n",
      " - 1s - loss: 2.0113 - acc: 0.2415 - val_loss: 2.0148 - val_acc: 0.2478\n",
      "Epoch 55/300\n",
      " - 1s - loss: 2.0121 - acc: 0.2415 - val_loss: 2.0160 - val_acc: 0.2478\n",
      "Epoch 56/300\n",
      " - 1s - loss: 2.0109 - acc: 0.2415 - val_loss: 2.0152 - val_acc: 0.2478\n",
      "Epoch 57/300\n",
      " - 1s - loss: 2.0110 - acc: 0.2411 - val_loss: 2.0150 - val_acc: 0.2478\n",
      "Epoch 58/300\n",
      " - 1s - loss: 2.0119 - acc: 0.2415 - val_loss: 2.0140 - val_acc: 0.2478\n",
      "Epoch 59/300\n",
      " - 1s - loss: 2.0127 - acc: 0.2415 - val_loss: 2.0139 - val_acc: 0.2478\n",
      "Epoch 60/300\n",
      " - 1s - loss: 2.0093 - acc: 0.2415 - val_loss: 2.0160 - val_acc: 0.2478\n",
      "Epoch 61/300\n",
      " - 1s - loss: 2.0109 - acc: 0.2415 - val_loss: 2.0154 - val_acc: 0.2478\n",
      "Epoch 62/300\n",
      " - 1s - loss: 2.0119 - acc: 0.2417 - val_loss: 2.0144 - val_acc: 0.2478\n",
      "Epoch 63/300\n",
      " - 1s - loss: 2.0118 - acc: 0.2415 - val_loss: 2.0143 - val_acc: 0.2478\n",
      "Epoch 64/300\n",
      " - 1s - loss: 2.0104 - acc: 0.2413 - val_loss: 2.0135 - val_acc: 0.2478\n",
      "Epoch 65/300\n",
      " - 1s - loss: 2.0104 - acc: 0.2407 - val_loss: 2.0157 - val_acc: 0.2478\n",
      "Epoch 66/300\n",
      " - 1s - loss: 2.0112 - acc: 0.2415 - val_loss: 2.0133 - val_acc: 0.2478\n",
      "Epoch 67/300\n",
      " - 1s - loss: 2.0107 - acc: 0.2413 - val_loss: 2.0153 - val_acc: 0.2478\n",
      "Epoch 68/300\n",
      " - 1s - loss: 2.0094 - acc: 0.2417 - val_loss: 2.0114 - val_acc: 0.2478\n",
      "Epoch 69/300\n",
      " - 1s - loss: 2.0093 - acc: 0.2417 - val_loss: 2.0131 - val_acc: 0.2478\n",
      "Epoch 70/300\n",
      " - 1s - loss: 2.0083 - acc: 0.2413 - val_loss: 2.0124 - val_acc: 0.2478\n",
      "Epoch 71/300\n",
      " - 1s - loss: 2.0071 - acc: 0.2417 - val_loss: 2.0079 - val_acc: 0.2478\n",
      "Epoch 72/300\n",
      " - 1s - loss: 2.0045 - acc: 0.2415 - val_loss: 2.0108 - val_acc: 0.2478\n",
      "Epoch 73/300\n",
      " - 1s - loss: 2.0063 - acc: 0.2419 - val_loss: 2.0091 - val_acc: 0.2460\n",
      "Epoch 74/300\n",
      " - 1s - loss: 2.0042 - acc: 0.2385 - val_loss: 2.0077 - val_acc: 0.2388\n",
      "Epoch 75/300\n",
      " - 1s - loss: 2.0028 - acc: 0.2409 - val_loss: 2.0141 - val_acc: 0.2478\n",
      "Epoch 76/300\n",
      " - 1s - loss: 2.0055 - acc: 0.2409 - val_loss: 2.0080 - val_acc: 0.2442\n",
      "Epoch 77/300\n",
      " - 1s - loss: 2.0039 - acc: 0.2405 - val_loss: 2.0058 - val_acc: 0.2442\n",
      "Epoch 78/300\n",
      " - 1s - loss: 2.0033 - acc: 0.2393 - val_loss: 2.0131 - val_acc: 0.2478\n",
      "Epoch 79/300\n",
      " - 1s - loss: 2.0029 - acc: 0.2417 - val_loss: 2.0076 - val_acc: 0.2460\n",
      "Epoch 80/300\n",
      " - 1s - loss: 2.0054 - acc: 0.2393 - val_loss: 2.0078 - val_acc: 0.2442\n",
      "Epoch 81/300\n",
      " - 1s - loss: 2.0031 - acc: 0.2407 - val_loss: 2.0067 - val_acc: 0.2478\n",
      "Epoch 82/300\n",
      " - 1s - loss: 2.0055 - acc: 0.2421 - val_loss: 2.0072 - val_acc: 0.2478\n",
      "Epoch 83/300\n",
      " - 1s - loss: 2.0026 - acc: 0.2419 - val_loss: 2.0084 - val_acc: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/300\n",
      " - 1s - loss: 2.0008 - acc: 0.2431 - val_loss: 2.0138 - val_acc: 0.2478\n",
      "Epoch 85/300\n",
      " - 1s - loss: 1.9999 - acc: 0.2415 - val_loss: 2.0073 - val_acc: 0.2442\n",
      "Epoch 86/300\n",
      " - 1s - loss: 2.0038 - acc: 0.2411 - val_loss: 2.0083 - val_acc: 0.2478\n",
      "Epoch 87/300\n",
      " - 1s - loss: 2.0018 - acc: 0.2429 - val_loss: 2.0081 - val_acc: 0.2478\n",
      "Epoch 88/300\n",
      " - 1s - loss: 2.0058 - acc: 0.2397 - val_loss: 2.0089 - val_acc: 0.2478\n",
      "Epoch 89/300\n",
      " - 1s - loss: 2.0031 - acc: 0.2431 - val_loss: 2.0080 - val_acc: 0.2478\n",
      "Epoch 90/300\n",
      " - 1s - loss: 2.0012 - acc: 0.2413 - val_loss: 2.0062 - val_acc: 0.2478\n",
      "Epoch 91/300\n",
      " - 1s - loss: 2.0034 - acc: 0.2421 - val_loss: 2.0086 - val_acc: 0.2478\n",
      "Epoch 92/300\n",
      " - 1s - loss: 2.0013 - acc: 0.2397 - val_loss: 2.0106 - val_acc: 0.2478\n",
      "Epoch 93/300\n",
      " - 1s - loss: 2.0017 - acc: 0.2419 - val_loss: 2.0103 - val_acc: 0.2478\n",
      "Epoch 94/300\n",
      " - 1s - loss: 2.0036 - acc: 0.2419 - val_loss: 2.0082 - val_acc: 0.2478\n",
      "Epoch 95/300\n",
      " - 1s - loss: 2.0002 - acc: 0.2421 - val_loss: 2.0062 - val_acc: 0.2424\n",
      "Epoch 00095: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd8fa5631d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergeData()\n",
    "train=readData()\n",
    "train=changeYear(train)\n",
    "train=augFeatures(train)\n",
    "train=manage(train)\n",
    "train=train.drop([\"最高價\"], axis=1)\n",
    "train=train.drop([\"最低價\"], axis=1)\n",
    "train=train.drop([\"收盤價\"], axis=1)\n",
    "train=train.drop([\"年\"], axis=1)\n",
    "train=train.drop([\"月\"], axis=1)\n",
    "train=train.drop([\"日\"], axis=1)\n",
    "train=train.drop([\"第幾日\"], axis=1)\n",
    "temp=train\n",
    "train=normalize(train)\n",
    "train_x1, train_y1 = buildTrain(train,5,1)\n",
    "train_x2, train_y2 = buildTrain(temp,5,1)\n",
    "train_x, train_y = train_x1,train_y2 \n",
    "train_y=np_utils.to_categorical(train_y)\n",
    "train_x, train_y = shuffle(train_x, train_y )\n",
    "train_x, train_y , test_x, test_y = splitData(train_x, train_y , 0.1)\n",
    "history = LossHistory()\n",
    "model = buildModel(train_x.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(train_x, train_y, epochs=300, batch_size=128, verbose=2,validation_split=0.1, callbacks=[callback,history])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618/618 [==============================] - 0s 133us/step\n",
      "test loss:  2.0106909707140384\n",
      "test accuracy:  0.2152103561799503\n"
     ]
    }
   ],
   "source": [
    "history.loss_plot('epoch')\n",
    "loss_1, accuracy_1 = model.evaluate(test_x, test_y)\n",
    "print('test loss: ', loss_1)\n",
    "print('test accuracy: ', accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\admin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\admin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, input_shape=(5, 5), return_sequences=True)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 5, 128)            68608     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 1161      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 201,353\n",
      "Trainable params: 201,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5010 samples, validate on 557 samples\n",
      "Epoch 1/300\n",
      " - 3s - loss: 2.0708 - acc: 0.2289 - val_loss: 1.9901 - val_acc: 0.1921\n",
      "Epoch 2/300\n",
      " - 1s - loss: 1.9815 - acc: 0.2371 - val_loss: 1.9661 - val_acc: 0.2298\n",
      "Epoch 3/300\n",
      " - 1s - loss: 1.9738 - acc: 0.2473 - val_loss: 1.9650 - val_acc: 0.2370\n",
      "Epoch 4/300\n",
      " - 1s - loss: 1.9714 - acc: 0.2489 - val_loss: 1.9655 - val_acc: 0.2406\n",
      "Epoch 5/300\n",
      " - 1s - loss: 1.9709 - acc: 0.2497 - val_loss: 1.9728 - val_acc: 0.2262\n",
      "Epoch 6/300\n",
      " - 1s - loss: 1.9679 - acc: 0.2515 - val_loss: 1.9607 - val_acc: 0.2442\n",
      "Epoch 7/300\n",
      " - 1s - loss: 1.9684 - acc: 0.2501 - val_loss: 1.9621 - val_acc: 0.2352\n",
      "Epoch 8/300\n",
      " - 1s - loss: 1.9661 - acc: 0.2495 - val_loss: 1.9637 - val_acc: 0.2442\n",
      "Epoch 9/300\n",
      " - 1s - loss: 1.9648 - acc: 0.2503 - val_loss: 1.9712 - val_acc: 0.2460\n",
      "Epoch 10/300\n",
      " - 1s - loss: 1.9639 - acc: 0.2513 - val_loss: 1.9673 - val_acc: 0.2370\n",
      "Epoch 11/300\n",
      " - 1s - loss: 1.9626 - acc: 0.2521 - val_loss: 1.9716 - val_acc: 0.2352\n",
      "Epoch 12/300\n",
      " - 1s - loss: 1.9622 - acc: 0.2511 - val_loss: 1.9685 - val_acc: 0.2334\n",
      "Epoch 13/300\n",
      " - 1s - loss: 1.9607 - acc: 0.2535 - val_loss: 1.9663 - val_acc: 0.2424\n",
      "Epoch 14/300\n",
      " - 1s - loss: 1.9595 - acc: 0.2553 - val_loss: 1.9671 - val_acc: 0.2460\n",
      "Epoch 15/300\n",
      " - 1s - loss: 1.9602 - acc: 0.2543 - val_loss: 1.9742 - val_acc: 0.2370\n",
      "Epoch 16/300\n",
      " - 1s - loss: 1.9582 - acc: 0.2583 - val_loss: 1.9672 - val_acc: 0.2513\n",
      "Epoch 17/300\n",
      " - 1s - loss: 1.9567 - acc: 0.2577 - val_loss: 1.9708 - val_acc: 0.2388\n",
      "Epoch 18/300\n",
      " - 1s - loss: 1.9597 - acc: 0.2507 - val_loss: 1.9660 - val_acc: 0.2460\n",
      "Epoch 19/300\n",
      " - 1s - loss: 1.9542 - acc: 0.2527 - val_loss: 1.9705 - val_acc: 0.2478\n",
      "Epoch 20/300\n",
      " - 1s - loss: 1.9545 - acc: 0.2547 - val_loss: 1.9730 - val_acc: 0.2460\n",
      "Epoch 21/300\n",
      " - 1s - loss: 1.9524 - acc: 0.2573 - val_loss: 1.9723 - val_acc: 0.2406\n",
      "Epoch 22/300\n",
      " - 1s - loss: 1.9530 - acc: 0.2531 - val_loss: 1.9662 - val_acc: 0.2442\n",
      "Epoch 23/300\n",
      " - 1s - loss: 1.9496 - acc: 0.2505 - val_loss: 1.9706 - val_acc: 0.2424\n",
      "Epoch 24/300\n",
      " - 1s - loss: 1.9495 - acc: 0.2513 - val_loss: 1.9751 - val_acc: 0.2442\n",
      "Epoch 25/300\n",
      " - 1s - loss: 1.9492 - acc: 0.2561 - val_loss: 1.9681 - val_acc: 0.2406\n",
      "Epoch 26/300\n",
      " - 1s - loss: 1.9459 - acc: 0.2577 - val_loss: 1.9704 - val_acc: 0.2424\n",
      "Epoch 27/300\n",
      " - 1s - loss: 1.9454 - acc: 0.2525 - val_loss: 1.9702 - val_acc: 0.2478\n",
      "Epoch 28/300\n",
      " - 1s - loss: 1.9470 - acc: 0.2553 - val_loss: 1.9659 - val_acc: 0.2478\n",
      "Epoch 29/300\n",
      " - 1s - loss: 1.9434 - acc: 0.2537 - val_loss: 1.9652 - val_acc: 0.2513\n",
      "Epoch 30/300\n",
      " - 1s - loss: 1.9414 - acc: 0.2563 - val_loss: 1.9594 - val_acc: 0.2424\n",
      "Epoch 31/300\n",
      " - 1s - loss: 1.9385 - acc: 0.2573 - val_loss: 1.9635 - val_acc: 0.2478\n",
      "Epoch 32/300\n",
      " - 1s - loss: 1.9389 - acc: 0.2569 - val_loss: 1.9655 - val_acc: 0.2585\n",
      "Epoch 33/300\n",
      " - 1s - loss: 1.9373 - acc: 0.2645 - val_loss: 1.9630 - val_acc: 0.2531\n",
      "Epoch 34/300\n",
      " - 1s - loss: 1.9380 - acc: 0.2565 - val_loss: 1.9663 - val_acc: 0.2496\n",
      "Epoch 35/300\n",
      " - 1s - loss: 1.9344 - acc: 0.2611 - val_loss: 1.9540 - val_acc: 0.2531\n",
      "Epoch 36/300\n",
      " - 1s - loss: 1.9322 - acc: 0.2633 - val_loss: 1.9555 - val_acc: 0.2567\n",
      "Epoch 37/300\n",
      " - 1s - loss: 1.9299 - acc: 0.2693 - val_loss: 1.9615 - val_acc: 0.2496\n",
      "Epoch 38/300\n",
      " - 1s - loss: 1.9294 - acc: 0.2639 - val_loss: 1.9648 - val_acc: 0.2621\n",
      "Epoch 39/300\n",
      " - 1s - loss: 1.9290 - acc: 0.2627 - val_loss: 1.9616 - val_acc: 0.2567\n",
      "Epoch 40/300\n",
      " - 1s - loss: 1.9236 - acc: 0.2645 - val_loss: 1.9565 - val_acc: 0.2424\n",
      "Epoch 41/300\n",
      " - 1s - loss: 1.9240 - acc: 0.2627 - val_loss: 1.9622 - val_acc: 0.2513\n",
      "Epoch 42/300\n",
      " - 1s - loss: 1.9215 - acc: 0.2617 - val_loss: 1.9585 - val_acc: 0.2513\n",
      "Epoch 43/300\n",
      " - 1s - loss: 1.9194 - acc: 0.2677 - val_loss: 1.9632 - val_acc: 0.2567\n",
      "Epoch 44/300\n",
      " - 1s - loss: 1.9193 - acc: 0.2627 - val_loss: 1.9691 - val_acc: 0.2460\n",
      "Epoch 45/300\n",
      " - 1s - loss: 1.9159 - acc: 0.2695 - val_loss: 1.9587 - val_acc: 0.2531\n",
      "Epoch 46/300\n",
      " - 1s - loss: 1.9144 - acc: 0.2653 - val_loss: 1.9681 - val_acc: 0.2496\n",
      "Epoch 47/300\n",
      " - 1s - loss: 1.9116 - acc: 0.2645 - val_loss: 1.9618 - val_acc: 0.2316\n",
      "Epoch 48/300\n",
      " - 1s - loss: 1.9097 - acc: 0.2679 - val_loss: 1.9673 - val_acc: 0.2603\n",
      "Epoch 49/300\n",
      " - 1s - loss: 1.9068 - acc: 0.2699 - val_loss: 1.9615 - val_acc: 0.2531\n",
      "Epoch 50/300\n",
      " - 1s - loss: 1.9037 - acc: 0.2693 - val_loss: 1.9700 - val_acc: 0.2496\n",
      "Epoch 51/300\n",
      " - 1s - loss: 1.9002 - acc: 0.2737 - val_loss: 1.9621 - val_acc: 0.2496\n",
      "Epoch 52/300\n",
      " - 1s - loss: 1.9025 - acc: 0.2731 - val_loss: 1.9686 - val_acc: 0.2567\n",
      "Epoch 53/300\n",
      " - 1s - loss: 1.8949 - acc: 0.2774 - val_loss: 1.9661 - val_acc: 0.2496\n",
      "Epoch 54/300\n",
      " - 1s - loss: 1.8927 - acc: 0.2733 - val_loss: 1.9627 - val_acc: 0.2549\n",
      "Epoch 55/300\n",
      " - 1s - loss: 1.8873 - acc: 0.2782 - val_loss: 1.9720 - val_acc: 0.2675\n",
      "Epoch 56/300\n",
      " - 1s - loss: 1.8908 - acc: 0.2752 - val_loss: 1.9627 - val_acc: 0.2549\n",
      "Epoch 57/300\n",
      " - 1s - loss: 1.8851 - acc: 0.2729 - val_loss: 1.9726 - val_acc: 0.2460\n",
      "Epoch 58/300\n",
      " - 1s - loss: 1.8859 - acc: 0.2786 - val_loss: 1.9738 - val_acc: 0.2406\n",
      "Epoch 59/300\n",
      " - 1s - loss: 1.8792 - acc: 0.2756 - val_loss: 1.9739 - val_acc: 0.2478\n",
      "Epoch 60/300\n",
      " - 1s - loss: 1.8783 - acc: 0.2778 - val_loss: 1.9814 - val_acc: 0.2531\n",
      "Epoch 61/300\n",
      " - 1s - loss: 1.8716 - acc: 0.2860 - val_loss: 1.9805 - val_acc: 0.2406\n",
      "Epoch 62/300\n",
      " - 1s - loss: 1.8627 - acc: 0.2822 - val_loss: 1.9774 - val_acc: 0.2496\n",
      "Epoch 63/300\n",
      " - 1s - loss: 1.8650 - acc: 0.2876 - val_loss: 1.9818 - val_acc: 0.2478\n",
      "Epoch 64/300\n",
      " - 1s - loss: 1.8613 - acc: 0.2894 - val_loss: 1.9868 - val_acc: 0.2424\n",
      "Epoch 65/300\n",
      " - 1s - loss: 1.8545 - acc: 0.2858 - val_loss: 1.9785 - val_acc: 0.2316\n",
      "Epoch 66/300\n",
      " - 1s - loss: 1.8521 - acc: 0.2884 - val_loss: 1.9804 - val_acc: 0.2513\n",
      "Epoch 67/300\n",
      " - 1s - loss: 1.8467 - acc: 0.2916 - val_loss: 1.9844 - val_acc: 0.2406\n",
      "Epoch 68/300\n",
      " - 1s - loss: 1.8378 - acc: 0.2922 - val_loss: 1.9884 - val_acc: 0.2406\n",
      "Epoch 69/300\n",
      " - 1s - loss: 1.8441 - acc: 0.2896 - val_loss: 1.9879 - val_acc: 0.2298\n",
      "Epoch 70/300\n",
      " - 1s - loss: 1.8349 - acc: 0.3076 - val_loss: 2.0032 - val_acc: 0.2154\n",
      "Epoch 71/300\n",
      " - 1s - loss: 1.8343 - acc: 0.2966 - val_loss: 1.9970 - val_acc: 0.2334\n",
      "Epoch 72/300\n",
      " - 1s - loss: 1.8236 - acc: 0.2942 - val_loss: 2.0036 - val_acc: 0.2370\n",
      "Epoch 73/300\n",
      " - 1s - loss: 1.8125 - acc: 0.3060 - val_loss: 2.0076 - val_acc: 0.2352\n",
      "Epoch 74/300\n",
      " - 1s - loss: 1.8107 - acc: 0.3054 - val_loss: 2.0013 - val_acc: 0.2262\n",
      "Epoch 75/300\n",
      " - 1s - loss: 1.8091 - acc: 0.2996 - val_loss: 2.0109 - val_acc: 0.2262\n",
      "Epoch 76/300\n",
      " - 1s - loss: 1.7952 - acc: 0.3088 - val_loss: 2.0074 - val_acc: 0.2136\n",
      "Epoch 77/300\n",
      " - 1s - loss: 1.7900 - acc: 0.3130 - val_loss: 2.0260 - val_acc: 0.2190\n",
      "Epoch 78/300\n",
      " - 1s - loss: 1.7897 - acc: 0.3240 - val_loss: 2.0193 - val_acc: 0.2190\n",
      "Epoch 79/300\n",
      " - 1s - loss: 1.7879 - acc: 0.3116 - val_loss: 2.0269 - val_acc: 0.2352\n",
      "Epoch 80/300\n",
      " - 1s - loss: 1.7724 - acc: 0.3146 - val_loss: 2.0306 - val_acc: 0.2154\n",
      "Epoch 81/300\n",
      " - 1s - loss: 1.7695 - acc: 0.3156 - val_loss: 2.0389 - val_acc: 0.2226\n",
      "Epoch 82/300\n",
      " - 1s - loss: 1.7606 - acc: 0.3244 - val_loss: 2.0475 - val_acc: 0.2334\n",
      "Epoch 83/300\n",
      " - 1s - loss: 1.7706 - acc: 0.3198 - val_loss: 2.0447 - val_acc: 0.2083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/300\n",
      " - 1s - loss: 1.7530 - acc: 0.3283 - val_loss: 2.0600 - val_acc: 0.2226\n",
      "Epoch 85/300\n",
      " - 1s - loss: 1.7487 - acc: 0.3349 - val_loss: 2.0624 - val_acc: 0.2136\n",
      "Epoch 86/300\n",
      " - 1s - loss: 1.7362 - acc: 0.3337 - val_loss: 2.0681 - val_acc: 0.2190\n",
      "Epoch 87/300\n",
      " - 1s - loss: 1.7299 - acc: 0.3337 - val_loss: 2.0667 - val_acc: 0.1975\n",
      "Epoch 88/300\n",
      " - 1s - loss: 1.7196 - acc: 0.3333 - val_loss: 2.0811 - val_acc: 0.2065\n",
      "Epoch 89/300\n",
      " - 1s - loss: 1.7122 - acc: 0.3457 - val_loss: 2.0898 - val_acc: 0.1939\n",
      "Epoch 90/300\n",
      " - 1s - loss: 1.6961 - acc: 0.3575 - val_loss: 2.0956 - val_acc: 0.2083\n",
      "Epoch 91/300\n",
      " - 1s - loss: 1.6984 - acc: 0.3597 - val_loss: 2.0958 - val_acc: 0.2101\n",
      "Epoch 92/300\n",
      " - 1s - loss: 1.6911 - acc: 0.3553 - val_loss: 2.1020 - val_acc: 0.2047\n",
      "Epoch 93/300\n",
      " - 1s - loss: 1.6777 - acc: 0.3611 - val_loss: 2.1106 - val_acc: 0.1957\n",
      "Epoch 94/300\n",
      " - 1s - loss: 1.6750 - acc: 0.3493 - val_loss: 2.1154 - val_acc: 0.2118\n",
      "Epoch 95/300\n",
      " - 1s - loss: 1.6677 - acc: 0.3619 - val_loss: 2.1266 - val_acc: 0.2190\n",
      "Epoch 96/300\n",
      " - 1s - loss: 1.6568 - acc: 0.3739 - val_loss: 2.1217 - val_acc: 0.2065\n",
      "Epoch 97/300\n",
      " - 1s - loss: 1.6496 - acc: 0.3733 - val_loss: 2.1501 - val_acc: 0.2101\n",
      "Epoch 98/300\n",
      " - 1s - loss: 1.6473 - acc: 0.3727 - val_loss: 2.1427 - val_acc: 0.2101\n",
      "Epoch 99/300\n",
      " - 1s - loss: 1.6339 - acc: 0.3846 - val_loss: 2.1513 - val_acc: 0.1957\n",
      "Epoch 100/300\n",
      " - 1s - loss: 1.6196 - acc: 0.3812 - val_loss: 2.1564 - val_acc: 0.1975\n",
      "Epoch 101/300\n",
      " - 1s - loss: 1.6152 - acc: 0.3844 - val_loss: 2.1775 - val_acc: 0.1975\n",
      "Epoch 102/300\n",
      " - 1s - loss: 1.6023 - acc: 0.3888 - val_loss: 2.1639 - val_acc: 0.2065\n",
      "Epoch 103/300\n",
      " - 1s - loss: 1.6019 - acc: 0.3884 - val_loss: 2.1747 - val_acc: 0.2136\n",
      "Epoch 104/300\n",
      " - 1s - loss: 1.5824 - acc: 0.4058 - val_loss: 2.1945 - val_acc: 0.2029\n",
      "Epoch 105/300\n",
      " - 1s - loss: 1.5769 - acc: 0.4024 - val_loss: 2.1919 - val_acc: 0.2065\n",
      "Epoch 106/300\n",
      " - 1s - loss: 1.5714 - acc: 0.4050 - val_loss: 2.2118 - val_acc: 0.2047\n",
      "Epoch 107/300\n",
      " - 1s - loss: 1.5599 - acc: 0.4076 - val_loss: 2.2102 - val_acc: 0.2172\n",
      "Epoch 108/300\n",
      " - 1s - loss: 1.5453 - acc: 0.4154 - val_loss: 2.2125 - val_acc: 0.2011\n",
      "Epoch 109/300\n",
      " - 1s - loss: 1.5356 - acc: 0.4212 - val_loss: 2.2090 - val_acc: 0.2172\n",
      "Epoch 110/300\n",
      " - 1s - loss: 1.5323 - acc: 0.4186 - val_loss: 2.2255 - val_acc: 0.2172\n",
      "Epoch 111/300\n",
      " - 1s - loss: 1.5264 - acc: 0.4210 - val_loss: 2.2383 - val_acc: 0.2083\n",
      "Epoch 112/300\n",
      " - 1s - loss: 1.5244 - acc: 0.4315 - val_loss: 2.2471 - val_acc: 0.2154\n",
      "Epoch 113/300\n",
      " - 1s - loss: 1.5030 - acc: 0.4291 - val_loss: 2.2526 - val_acc: 0.2011\n",
      "Epoch 114/300\n",
      " - 1s - loss: 1.4890 - acc: 0.4387 - val_loss: 2.2598 - val_acc: 0.2065\n",
      "Epoch 115/300\n",
      " - 1s - loss: 1.4955 - acc: 0.4389 - val_loss: 2.2773 - val_acc: 0.2101\n",
      "Epoch 116/300\n",
      " - 1s - loss: 1.4839 - acc: 0.4387 - val_loss: 2.2946 - val_acc: 0.2208\n",
      "Epoch 117/300\n",
      " - 1s - loss: 1.4810 - acc: 0.4415 - val_loss: 2.2940 - val_acc: 0.1957\n",
      "Epoch 118/300\n",
      " - 1s - loss: 1.4705 - acc: 0.4361 - val_loss: 2.2945 - val_acc: 0.2118\n",
      "Epoch 119/300\n",
      " - 1s - loss: 1.4471 - acc: 0.4527 - val_loss: 2.3204 - val_acc: 0.1957\n",
      "Epoch 120/300\n",
      " - 1s - loss: 1.4485 - acc: 0.4501 - val_loss: 2.3193 - val_acc: 0.2011\n",
      "Epoch 121/300\n",
      " - 1s - loss: 1.4414 - acc: 0.4551 - val_loss: 2.3373 - val_acc: 0.1975\n",
      "Epoch 122/300\n",
      " - 1s - loss: 1.4329 - acc: 0.4537 - val_loss: 2.3484 - val_acc: 0.2011\n",
      "Epoch 123/300\n",
      " - 1s - loss: 1.4110 - acc: 0.4739 - val_loss: 2.3524 - val_acc: 0.1867\n",
      "Epoch 124/300\n",
      " - 1s - loss: 1.4148 - acc: 0.4673 - val_loss: 2.3655 - val_acc: 0.1957\n",
      "Epoch 125/300\n",
      " - 1s - loss: 1.4055 - acc: 0.4639 - val_loss: 2.3737 - val_acc: 0.1921\n",
      "Epoch 126/300\n",
      " - 1s - loss: 1.3928 - acc: 0.4723 - val_loss: 2.3782 - val_acc: 0.2118\n",
      "Epoch 127/300\n",
      " - 1s - loss: 1.3814 - acc: 0.4735 - val_loss: 2.3834 - val_acc: 0.2029\n",
      "Epoch 128/300\n",
      " - 1s - loss: 1.3789 - acc: 0.4794 - val_loss: 2.3956 - val_acc: 0.2029\n",
      "Epoch 129/300\n",
      " - 1s - loss: 1.3491 - acc: 0.4860 - val_loss: 2.4102 - val_acc: 0.1975\n",
      "Epoch 130/300\n",
      " - 1s - loss: 1.3506 - acc: 0.4962 - val_loss: 2.4130 - val_acc: 0.2065\n",
      "Epoch 131/300\n",
      " - 1s - loss: 1.3563 - acc: 0.4876 - val_loss: 2.4247 - val_acc: 0.1831\n",
      "Epoch 132/300\n",
      " - 1s - loss: 1.3325 - acc: 0.5018 - val_loss: 2.4366 - val_acc: 0.1993\n",
      "Epoch 133/300\n",
      " - 1s - loss: 1.3285 - acc: 0.5026 - val_loss: 2.4354 - val_acc: 0.1867\n",
      "Epoch 134/300\n",
      " - 1s - loss: 1.3252 - acc: 0.5060 - val_loss: 2.4540 - val_acc: 0.1939\n",
      "Epoch 135/300\n",
      " - 1s - loss: 1.3190 - acc: 0.4998 - val_loss: 2.4590 - val_acc: 0.1939\n",
      "Epoch 136/300\n"
     ]
    }
   ],
   "source": [
    "mergeData()\n",
    "train=readData()\n",
    "train=changeYear(train)\n",
    "train=augFeatures(train)\n",
    "train=manage(train)\n",
    "train=train.drop([\"最高價\"], axis=1)\n",
    "train=train.drop([\"最低價\"], axis=1)\n",
    "train=train.drop([\"收盤價\"], axis=1)\n",
    "temp=train\n",
    "train=normalize(train)\n",
    "train_x1, train_y1 = buildTrain(train,5,1)\n",
    "train_x2, train_y2 = buildTrain(temp,5,1)\n",
    "train_x, train_y = train_x1,train_y2 \n",
    "train_y=np_utils.to_categorical(train_y)\n",
    "train_x, train_y = shuffle(train_x, train_y )\n",
    "train_x, train_y , test_x, test_y = splitData(train_x, train_y , 0.1)\n",
    "history = LossHistory()\n",
    "model = buildModel(train_x.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(train_x, train_y, epochs=300, batch_size=128, verbose=2,validation_split=0.1, callbacks=[callback,history])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.loss_plot('epoch')\n",
    "loss_2, accuracy_2 = model.evaluate(test_x, test_y)\n",
    "print('test loss: ', loss_2)\n",
    "print('test accuracy: ', accuracy_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeData()\n",
    "train=readData()\n",
    "train=changeYear(train)\n",
    "train=augFeatures(train)\n",
    "train=manage(train)\n",
    "train=train.drop([\"收盤價\"], axis=1)\n",
    "temp=train\n",
    "train=normalize(train)\n",
    "train_x1, train_y1 = buildTrain(train,5,1)\n",
    "train_x2, train_y2 = buildTrain(temp,5,1)\n",
    "train_x, train_y = train_x1,train_y2 \n",
    "train_y=np_utils.to_categorical(train_y)\n",
    "train_x, train_y = shuffle(train_x, train_y )\n",
    "train_x, train_y , test_x, test_y = splitData(train_x, train_y , 0.1)\n",
    "history = LossHistory()\n",
    "model = buildModel(train_x.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(train_x, train_y, epochs=300, batch_size=128, verbose=2,validation_split=0.1, callbacks=[callback,history])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.loss_plot('epoch')\n",
    "loss_3, accuracy_3 = model.evaluate(test_x, test_y)\n",
    "print('test loss: ', loss_3)\n",
    "print('test accuracy: ', accuracy_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeData()\n",
    "train=readData()\n",
    "train=changeYear(train)\n",
    "train=augFeatures(train)\n",
    "train=manage(train)\n",
    "temp=train\n",
    "train=normalize(train)\n",
    "train_x1, train_y1 = buildTrain(train,5,1)\n",
    "train_x2, train_y2 = buildTrain(temp,5,1)\n",
    "train_x, train_y = train_x1,train_y2 \n",
    "train_y=np_utils.to_categorical(train_y)\n",
    "train_x, train_y = shuffle(train_x, train_y )\n",
    "train_x, train_y , test_x, test_y = splitData(train_x, train_y , 0.1)\n",
    "history = LossHistory()\n",
    "model = buildModel(train_x.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(train_x, train_y, epochs=300, batch_size=128, verbose=2,validation_split=0.1, callbacks=[callback,history])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.loss_plot('epoch')\n",
    "loss_4, accuracy_4 = model.evaluate(test_x, test_y)\n",
    "print('test loss: ', loss_4)\n",
    "print('test accuracy: ', accuracy_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=[accuracy_1*100,accuracy_2*100,accuracy_3*100,accuracy_4*100]\n",
    "number=[1,2,3,4]\n",
    "plt.plot(number, accuracy)\n",
    "plt.xlabel(\"accuracy\")\n",
    "plt.ylabel(\"percent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
