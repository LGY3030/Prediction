{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把所有年份的資料merge在一起,並存在data.csv裡\n",
    "def mergeData():\n",
    "    SaveFile_Name = 'data.csv'\n",
    "    file_list = os.listdir('data')\n",
    "    df = pd.read_csv('data'+'\\\\'+file_list[0])\n",
    "    df.to_csv(SaveFile_Name,encoding=\"utf_8_sig\",index=False)\n",
    "    for i in range(1,len(file_list)):\n",
    "        df = pd.read_csv('data'+'\\\\'+file_list[i])\n",
    "        df.to_csv(SaveFile_Name,encoding=\"utf_8_sig\",index=False, header=False, mode='a+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取data.csv\n",
    "def readData():\n",
    "    train = pd.read_csv(\"data.csv\")\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把年份換成西元年\n",
    "def changeYear(data):\n",
    "    for i in range(0,data.shape[0]):\n",
    "        Date=data[\"日期\"][i].split('/')\n",
    "        year,month,date=Date[0],Date[1],Date[2]\n",
    "        year=str(int(year)+1911)\n",
    "        data.loc[i,\"日期\"]=year+'/'+month+'/'+date\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加features(\"年\",\"月\",\"日\",\"第幾日\")\n",
    "def augFeatures(data):\n",
    "  data[\"日期\"] = pd.to_datetime(data[\"日期\"])\n",
    "  data[\"年\"] = data[\"日期\"].dt.year\n",
    "  data[\"月\"] = data[\"日期\"].dt.month\n",
    "  data[\"日\"] = data[\"日期\"].dt.day\n",
    "  data[\"第幾日\"] = data[\"日期\"].dt.dayofweek\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把非數字的資料換成正確資料,並減少features(\"日期\",\"成交股數\",\"成交金額\",等等...)\n",
    "def manage(data):\n",
    "    for i in range(0,data.shape[0]):\n",
    "        if data[\"漲跌價差\"][i]=='X0.00':\n",
    "            data.loc[i,\"漲跌價差\"]=str(int(data[\"收盤價\"][i])-int(data[\"收盤價\"][i-1]))\n",
    "    data=data.drop([\"日期\"], axis=1)\n",
    "    data=data.drop([\"成交股數\"], axis=1)\n",
    "    data=data.drop([\"成交金額\"], axis=1)\n",
    "    data=data.drop([\"漲跌價差\"], axis=1)\n",
    "    data=data.drop([\"成交筆數\"], axis=1)\n",
    "    data=data.convert_objects(convert_numeric=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把資料normalize\n",
    "def normalize(train):\n",
    "    train = train.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創造出train的資料,train_x為輸入資料(所有features),train_y為輸出資料(開盤價的成長率,分為9個區段)\n",
    "def buildTrain(train, pastDay=15, futureDay=1):\n",
    "    X_train, Y_train, Z_train= [], [], []\n",
    "    X,Y,Z=[],[],[]\n",
    "    for i in range(train.shape[0]-futureDay-pastDay):\n",
    "        X_train.append(np.array(train.iloc[i:i+pastDay]))\n",
    "        Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][\"開盤價\"]))\n",
    "        Z_train.append(np.array(train.iloc[i+pastDay-1:i+pastDay][\"開盤價\"]))\n",
    "    X=np.array(X_train)\n",
    "    Y=np.array(Y_train)\n",
    "    Z=np.array(Z_train)\n",
    "    Y=100*((Y-Z)/Z)\n",
    "    Y_train=[]\n",
    "    \n",
    "    for i in range(len(Y)):\n",
    "        if Y[i]<-3.5:\n",
    "            Y_train.append(np.array([0]))\n",
    "        elif -3.5<=Y[i]<-2.5:\n",
    "            Y_train.append(np.array([1]))\n",
    "        elif -2.5<=Y[i]<-1.5:\n",
    "            Y_train.append(np.array([2]))\n",
    "        elif -1.5<=Y[i]<-0.5:\n",
    "            Y_train.append(np.array([3]))\n",
    "        elif -0.5<=Y[i]<0.5:\n",
    "            Y_train.append(np.array([4]))\n",
    "        elif 0.5<=Y[i]<1.5:\n",
    "            Y_train.append(np.array([5]))\n",
    "        elif 1.5<=Y[i]<2.5:\n",
    "            Y_train.append(np.array([6]))\n",
    "        elif 2.5<=Y[i]<3.5:\n",
    "            Y_train.append(np.array([7]))\n",
    "        elif 3.5<=Y[i]:\n",
    "            Y_train.append(np.array([8]))\n",
    "    Y=np.array(Y_train)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把資料打亂\n",
    "def shuffle(X,Y):\n",
    "    np.random.seed()\n",
    "    randomList = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randomList)\n",
    "    return X[randomList], Y[randomList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將資料分成訓練資料和測試資料\n",
    "def splitData(X,Y,rate):\n",
    "    X_train = X[:-int(X.shape[0]*rate)]\n",
    "\n",
    "    Y_train = Y[:-int(Y.shape[0]*rate)]\n",
    "    \n",
    "    X_val = X[-int(X.shape[0]*rate):]\n",
    "\n",
    "    Y_val = Y[-int(Y.shape[0]*rate):]\n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立模型\n",
    "def buildModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, input_length=shape[1], input_dim=shape[2],return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(200))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(9)) \n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='adam',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    " \n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\admin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\admin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(200, return_sequences=True, input_shape=(15, 1))`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 15, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9)                 1809      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 484,209\n",
      "Trainable params: 484,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5002 samples, validate on 556 samples\n",
      "Epoch 1/300\n",
      " - 9s - loss: 2.0526 - acc: 0.2189 - val_loss: 2.0012 - val_acc: 0.2428\n",
      "Epoch 2/300\n",
      " - 6s - loss: 2.0257 - acc: 0.2337 - val_loss: 1.9949 - val_acc: 0.2428\n",
      "Epoch 3/300\n",
      " - 6s - loss: 2.0216 - acc: 0.2349 - val_loss: 2.0079 - val_acc: 0.2428\n",
      "Epoch 4/300\n"
     ]
    }
   ],
   "source": [
    "mergeData()\n",
    "train=readData()\n",
    "train=changeYear(train)\n",
    "train=augFeatures(train)\n",
    "train=manage(train)\n",
    "train=train.drop([\"最高價\"], axis=1)\n",
    "train=train.drop([\"最低價\"], axis=1)\n",
    "train=train.drop([\"收盤價\"], axis=1)\n",
    "train=train.drop([\"年\"], axis=1)\n",
    "train=train.drop([\"月\"], axis=1)\n",
    "train=train.drop([\"日\"], axis=1)\n",
    "train=train.drop([\"第幾日\"], axis=1)\n",
    "temp=train\n",
    "train=normalize(train)\n",
    "train_x1, train_y1 = buildTrain(train,15,1)\n",
    "train_x2, train_y2 = buildTrain(temp,15,1)\n",
    "train_x, train_y = train_x1,train_y2 \n",
    "train_y=np_utils.to_categorical(train_y)\n",
    "train_x, train_y = shuffle(train_x, train_y )\n",
    "train_x, train_y , test_x, test_y = splitData(train_x, train_y , 0.1)\n",
    "history = LossHistory()\n",
    "model = buildModel(train_x.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(train_x, train_y, epochs=300, batch_size=128, verbose=2,validation_split=0.1, callbacks=[callback,history])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.loss_plot('epoch')\n",
    "loss_1, accuracy_1 = model.evaluate(test_x, test_y)\n",
    "print('test loss: ', loss_1)\n",
    "print('test accuracy: ', accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeData()\n",
    "train=readData()\n",
    "train=changeYear(train)\n",
    "train=augFeatures(train)\n",
    "train=manage(train)\n",
    "train=train.drop([\"最高價\"], axis=1)\n",
    "train=train.drop([\"最低價\"], axis=1)\n",
    "train=train.drop([\"收盤價\"], axis=1)\n",
    "temp=train\n",
    "train=normalize(train)\n",
    "train_x1, train_y1 = buildTrain(train,15,1)\n",
    "train_x2, train_y2 = buildTrain(temp,15,1)\n",
    "train_x, train_y = train_x1,train_y2 \n",
    "train_y=np_utils.to_categorical(train_y)\n",
    "train_x, train_y = shuffle(train_x, train_y )\n",
    "train_x, train_y , test_x, test_y = splitData(train_x, train_y , 0.1)\n",
    "history = LossHistory()\n",
    "model = buildModel(train_x.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(train_x, train_y, epochs=300, batch_size=128, verbose=2,validation_split=0.1, callbacks=[callback,history])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.loss_plot('epoch')\n",
    "loss_2, accuracy_2 = model.evaluate(test_x, test_y)\n",
    "print('test loss: ', loss_2)\n",
    "print('test accuracy: ', accuracy_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeData()\n",
    "train=readData()\n",
    "train=changeYear(train)\n",
    "train=augFeatures(train)\n",
    "train=manage(train)\n",
    "train=train.drop([\"收盤價\"], axis=1)\n",
    "temp=train\n",
    "train=normalize(train)\n",
    "train_x1, train_y1 = buildTrain(train,15,1)\n",
    "train_x2, train_y2 = buildTrain(temp,15,1)\n",
    "train_x, train_y = train_x1,train_y2 \n",
    "train_y=np_utils.to_categorical(train_y)\n",
    "train_x, train_y = shuffle(train_x, train_y )\n",
    "train_x, train_y , test_x, test_y = splitData(train_x, train_y , 0.1)\n",
    "history = LossHistory()\n",
    "model = buildModel(train_x.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(train_x, train_y, epochs=300, batch_size=128, verbose=2,validation_split=0.1, callbacks=[callback,history])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.loss_plot('epoch')\n",
    "loss_3, accuracy_3 = model.evaluate(test_x, test_y)\n",
    "print('test loss: ', loss_3)\n",
    "print('test accuracy: ', accuracy_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeData()\n",
    "train=readData()\n",
    "train=changeYear(train)\n",
    "train=augFeatures(train)\n",
    "train=manage(train)\n",
    "temp=train\n",
    "train=normalize(train)\n",
    "train_x1, train_y1 = buildTrain(train,15,1)\n",
    "train_x2, train_y2 = buildTrain(temp,15,1)\n",
    "train_x, train_y = train_x1,train_y2 \n",
    "train_y=np_utils.to_categorical(train_y)\n",
    "train_x, train_y = shuffle(train_x, train_y )\n",
    "train_x, train_y , test_x, test_y = splitData(train_x, train_y , 0.1)\n",
    "history = LossHistory()\n",
    "model = buildModel(train_x.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(train_x, train_y, epochs=300, batch_size=128, verbose=2,validation_split=0.1, callbacks=[callback,history])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.loss_plot('epoch')\n",
    "loss_4, accuracy_4 = model.evaluate(test_x, test_y)\n",
    "print('test loss: ', loss_4)\n",
    "print('test accuracy: ', accuracy_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=[accuracy_1*100,accuracy_2*100,accuracy_3*100,accuracy_4*100]\n",
    "number=[1,2,3,4]\n",
    "plt.plot(number, accuracy)\n",
    "plt.xlabel(\"accuracy\")\n",
    "plt.ylabel(\"percent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
